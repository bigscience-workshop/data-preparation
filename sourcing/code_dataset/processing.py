import gzip
import multiprocessing
import os
import shutil
import time
from argparse import Namespace
from collections import Counter
import numpy as np
from datasets import load_dataset
import re

raise ValueError("""\
This script has a known bug and will filter only for GPL licenses instead of removing them. \
It is provided as is such that the training data can be reproduced, but we strongly advise fixing it. \
The code can be found in the function `filter_licenses` on line 92.\
""")

PATTERN = re.compile(r'\s+')
LICENSES = {"lgpl-3.0", "agpl-3.0", "lgpl-2.1", "gpl-3.0", "gpl-2.0"}


def get_hash(example):
    """Get hash of content field."""
    return {"hash": hash(re.sub(PATTERN, '', example["content"]))}


def line_stats(example):
    """Calculates mean and max line length of file."""
    line_lengths = [len(line) for line in example["content"].splitlines()]
    return {"line_mean": np.mean(line_lengths), "line_max": max(line_lengths)}


def alpha_stats(example):
    """Calculates mean and max line length of file."""
    alpha_frac = np.mean([c.isalnum() for c in example["content"]])
    return {"alpha_frac": alpha_frac}


def check_uniques(example, uniques):
    """Check if current hash is still in set of unique hashes and remove if true."""
    if example["hash"] in uniques:
        uniques.remove(example["hash"])
        return True
    else:
        return False


def is_autogenerated(example, scan_width=5):
    """Check if file is autogenerated by looking for keywords in the first few lines of the file."""
    keywords = ["auto-generated", "autogenerated", "automatically generated"]
    lines = example["content"].splitlines()
    for _, line in zip(range(scan_width), lines):
        for keyword in keywords:
            if keyword in line.lower():
                return {"autogenerated": True}
    else:
        return {"autogenerated": False}


def preprocess(example):
    """Chain all preprocessing steps into one function to not fill cache."""
    results = dict()
    results.update(get_hash(example))
    results.update(line_stats(example))
    return results


def filter(example, uniques, args):
    """Filter dataset with heuristics."""
    if not check_uniques(example, uniques):
        return False
    elif example["line_max"] > args.line_max:
        return False
    else:
        return True

def filter_token_len_avg_std(examples):
    length_distributions = [[len(token) for token in text.split()] for text in examples["text"]]
    return [np.std(length_distribution) > 3 for length_distribution in length_distributions]

def filter_text_len(examples):
    return [100 < len(text) < 200000 for text in examples["text"]]

def filter_special_character_ratio(examples):
    return [0.15 < 1 - (len(re.findall('[\w]', text)) / len(text)) < 0.65 for text in examples["text"]]

def filter_longest_line(examples):
    return [20 < max([len(line) for line in text.split("\n")]) < 1000 for text in examples["text"]]

def filter_licences(examples):
    # THIS IS A BUG!!!
    return [license in LICENSES for license in examples["license"]]
    # TO FIX REPLACE WITH THE FOLLOWING LINE:
    # return [license in LICENSES for license not in examples["license"]]

def filter_by_all(examples):
    features = filter_token_len_avg_std(examples), filter_text_len(examples), filter_special_character_ratio(examples), filter_longest_line(examples), filter_licences(examples)
    return np.logical_and(np.logical_and(np.logical_and(features[0], features[1]), np.logical_and(features[2], features[3])), features[4])

def setup_meta(example):
    meta_dict = {}
    meta_dict["repo_name"] = example["repo_name"]
    meta_dict["path"] = example["path"]
    meta_dict["license"] = example["license"]
    return {"meta": str(meta_dict)}

# Settings
config = {
    "dataset_name": "data/github-alphacode",
    "num_workers": 30,
    "line_max": 1000,
    "line_mean": 100, "alpha_frac": 0.25}

args = Namespace(**config)

# Load dataset
t_start = time.time()
ds = load_dataset(args.dataset_name, split="train", chunksize=40<<20)
print(f"Time to load dataset: {time.time()-t_start:.2f}")

# Run preprocessing
t_start = time.time()
ds = ds.map(preprocess, num_proc=args.num_workers)
print(f"Time to preprocess dataset: {time.time()-t_start:.2f}")
print(ds)

# Deduplicate hashes
uniques = set(ds.unique("hash"))
frac = len(uniques) / len(ds)
print(f"Fraction of duplicates: {1-frac:.2%}")

# Deduplicate data and apply heuristics
t_start = time.time()
ds_filter = ds.filter(filter, fn_kwargs={"uniques": uniques, "args": args})
ds_filter = ds_filter.remove_columns(["line_mean", "line_max", "copies", "hash"])
print(f"Time to filter dataset: {time.time()-t_start:.2f}")
print(f"Size of filtered dataset: {len(ds_filter)}")

# Teven part
t_start = time.time()
ds_filter = ds_filter.rename_column("content", "text")
ds_filter_plus = ds_filter.filter(filter_by_all, batched=True, batch_size=1000, num_proc=args.num_workers)
ds_filter_plus = ds_filter_plus.map(lambda x: setup_meta(x), num_proc=args.num_workers)
ds_filter_plus = ds_filter_plus.remove_columns(["repo_name", "path", "license", "size"])
print(f"Time to filter dataset: {time.time()-t_start:.2f}")
print(f"Size of filtered dataset: {len(ds_filter_plus)}")

t_start = time.time()
ds_filter_plus.save_to_disk("./data/lm_code_github-clean")
#ds_filter_plus.push_to_hub("lm_code_github-clean")
print(f"Time to save: {time.time()-t_start:.2f}")